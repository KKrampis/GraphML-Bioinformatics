{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs and and machine learning for protein function prediction \n",
    "\n",
    "* Building the graph instance from the experimental data (i.e. PPI network if protein interactions)\n",
    "* Loading the graph entities (nodes mainly) with multidimensional data in order to enable predictions\n",
    "* Running machine learning algorithms on the graph for protein function inference on the unknown nodes\n",
    "\n",
    "---\n",
    "\n",
    "### Prepare file with network data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!wc -l 15761153-cut.tsv\n",
    "!cut -f 1-2,17-22 ./content/15761153.txt | head -n 2\n",
    "!cut -f 1-2,19-20./content/15761153.txt > ./content/15761153-cut.tsv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Python list slicing and lambda functions\n",
    "\n",
    "* Multiple tutorials can found online on these topics, example links are given here\n",
    "* On lambdas, see [here](https://cs.stanford.edu/people/nick/py/python-map-lambda.html) and [here](https://www.geeksforgeeks.org/python-lambda-anonymous-functions-filter-map-reduce/)\n",
    "* For list slicing see [here](https://www.geeksforgeeks.org/python-list-slicing/) and [here](https://stackoverflow.com/questions/509211/understanding-slice-notation)\n",
    "* Read material on links above !\n",
    "\n",
    "---\n",
    "### Let's parse out the file and make a Graph with NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./BioinformAI/content/15761153-cut.tsv', 'r') as f:\n",
    "  graphdata = [line.rstrip().split('\\t') for line in f]\n",
    "\n",
    "print(graphdata[0:3])\n",
    "\n",
    "graphdataclean = []\n",
    "\n",
    "for row in graphdata[1:]:\n",
    "  graphdataclean.append(list(map(lambda element: element[10:], row[0:2])) \\\n",
    "                      + list(map(lambda element: element[17:21], row[2:])))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(graphdataclean[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Let's parse out the file and make a Graph with NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes = []\n",
    "\n",
    "for row in graphdataclean:\n",
    "\tnodes.append( (row[0],{'type' : row[2]}) )\n",
    "\tnodes.append( (row[1],{'type' : row[3]}) )\n",
    "\n",
    "edges = []\n",
    "for row in graphdataclean:\n",
    "\tedges.append( (row[0],row[1]) )\n",
    "\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "labels = nx.get_node_attributes(G, 'type') \n",
    "nx.draw_networkx(G, labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we will make a list / dictionary with the node attributes (the prey-bait)\n",
    "* Then an array with the edges. Refer to Networkx [documentation](https://networkx.org/documentation/stable/tutorial.html) for [example](https://i.ibb.co/0C0XZ48/Screenshot-2022-02-14-9-44-36-PM.png)\n",
    "\n",
    "---\n",
    "\n",
    "### A smaller graph to visualize easier\n",
    "\n",
    "* Possibly some artefacts of nodes missing labels due to the slicing below\n",
    "* Open question : how does \"prey-bait\" node connections in graph correspond to experimental design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "nodesub = nodes[1:50]\n",
    "edgesub = edges[1:50]\n",
    "\n",
    "subG = nx.Graph()\n",
    "subG.add_nodes_from(nodesub)\n",
    "subG.add_edges_from(edgesub)\n",
    "labels = nx.get_node_attributes(subG, 'type') \n",
    "nx.draw_networkx(subG, labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### More dimensions in the node data for machine predictions\n",
    "\n",
    "![](https://i.ibb.co/vLbRWPj/Screenshot-2022-02-15-7-17-28-PM.png)\n",
    "\n",
    "* Each node has a single parameter (a single dimension) of data associated with it (prey or bait)\n",
    "* The more data, the better for machine learning predictions of unknown nodes\n",
    "* The annotations field has complex function description entered by bioinformatics analysts / curators\n",
    "\n",
    "---\n",
    "\n",
    "### How do we code human writen text for machine predictions\n",
    "\n",
    "* We need to codify the text as [one-hot encoded (1's and 0's) vectors](https://medium.com/analytics-vidhya/one-hot-encoding-of-text-data-in-natural-language-processing-2242fefb2148)\n",
    "\n",
    "![](https://i.ibb.co/pyG53Dd/Screenshot-2022-02-15-7-36-26-PM.png)\n",
    "\n",
    "* In the example above there are 7-dimensional vectors, each word in the node annotations represents a separate dimension \n",
    "* Annotations can have hundreds or thousands of dimensions, one for each word\n",
    "\n",
    "---\n",
    "\n",
    "### How do we code human writen text for machine predictions\n",
    "\n",
    "* Capturing in the vectors the context (surrounding) of each word for defining meaning precisely\n",
    "* [Natural Languange Processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) for human language data mining\n",
    "* For example: **\"We went to the river bank\"**\n",
    "* **\"We made a deposit at the bank\"**\n",
    "* Difference in meaning of word \"bank\" needs to be modeled in data for machine learning / AI applications\n",
    "\n",
    "---\n",
    "\n",
    "### Back to bioinformatics !\n",
    "\n",
    "* Our focus is how to add the rich annotation data on each node on our graphs\n",
    "* Add in a vectorized, one-hot encoded format data on each node, not focus on NLP\n",
    "* We will use algorithms and Python libraries such as [Word2vec](https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28) also [here](https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/) and [Gensim](https://www.machinelearningplus.com/nlp/gensim-tutorial/) from NLP\n",
    "* Once the graph is in place and one-hot encoded data on the nodes, we will run machine learning algorithms on it\n",
    "* Please read carefully the content from the links above !\n",
    "\n",
    "---\n",
    "\n",
    "### Bag of words representation\n",
    "\n",
    "* Create a dictionary - the set of unique words - from the sentences (or the \"corpus\")\n",
    "* Parse the sentences and create a one-hot encoded vector from each sentence:\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*hLvya7MXjsSc3NS2SoLMEg.png)\n",
    "[Image credit](https://www.ronaldjamesgroup.com/blog/grab-your-wine-its-time-to-demystify-ml-and-nlp)\n",
    "\n",
    "* Easy to implement, but a lot of wasted space, context not differentiated (\"bank deposit\" vs \"river bank\")\n",
    "\n",
    "---\n",
    "\n",
    "### Bag of words representation\n",
    "W\n",
    "* Create a dictionary - the set of unique word counts - from the annotations (or the \"corpus\")\n",
    "* Then  create a one-hot encoded group of vectors - multidimensional matrix (or tensor) attached to each node\n",
    "* First lets have our files with protein ids and annotations as accompanying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f 1-2,26-27 ./content/15761153.txt | head -n 2\n",
    "!cut -f 1-2,26-27 ./content/15761153.txt > ./content/15761153-cut.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's parse out the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./BioinformAI/content/15761153-cut.tsv', 'r') as f:\n",
    "  graphdata = [line.rstrip().split('\\t') for line in f]\n",
    "\n",
    "graphdataclean = []\n",
    "\n",
    "for row in graphdata[1:]:\n",
    "  graphdataclean.append(list(map(lambda element: element[10:], row[0:2])) \\\n",
    "                      + list(map(lambda element: element[22:], row[2:])))\n",
    "\n",
    "print(graphdataclean[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's one-hot encode the annotations\n",
    "\n",
    "* Do not forget to run \"!pip install gensim\"\n",
    "* We will follow examples from [this tutorial](https://www.machinelearningplus.com/nlp/gensim-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "corpus=[]\n",
    "for row in graphdataclean:\n",
    "\tcorpus.append(row[2])\n",
    "\tcorpus.append(row[3])\n",
    "\n",
    "print(corpus[0:2])\n",
    "\n",
    "annotwords = [[word for word in annotation.split()] for annotation in corpus]\n",
    "annotdict = corpora.Dictionary(annotwords)\n",
    "\n",
    "print(annotwords[0:2])\n",
    "print(annotdict.token2id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
