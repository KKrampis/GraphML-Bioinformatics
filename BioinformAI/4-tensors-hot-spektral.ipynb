{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### How do we code human writen text for machine predictions\n",
    "\n",
    "* We need to codify the text as [one-hot encoded (1's and 0's) vectors](https://medium.com/analytics-vidhya/one-hot-encoding-of-text-data-in-natural-language-processing-2242fefb2148)\n",
    "\n",
    "![](https://i.ibb.co/pyG53Dd/Screenshot-2022-02-15-7-36-26-PM.png)\n",
    "\n",
    "* In the example above there are 7-dimensional vectors, each word in the node annotations represents a separate dimension \n",
    "* Annotations can have hundreds or thousands of dimensions, one for each word\n",
    "\n",
    "---\n",
    "\n",
    "### An example with simplistic protein annotations\n",
    "\n",
    "![](https://i.ibb.co/ZSs4NGz/Screenshot-2022-02-19-7-38-54-PM.png)\n",
    "\n",
    "* Annotations with 5 words, the dictionary provides index for matching the 1's with the set of words\n",
    "* Tensor: 3-dimensional matrix, (annotation length) x (size of word set) x (number of annotations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Our tensor building \"algorithm\"\n",
    "\n",
    "* After having our dictionary index in place, go over each annotation sentence\n",
    "* Split the sentence in words, and for each word get its position on the index\n",
    "* Remember a tensor is a collection of matrices, each matrix correspond to one annotation\n",
    "* On each matrix each row designates the position of the word on the index\n",
    "* On each matrix we have as many rows as there are words (here our annotations are of same length)\n",
    "\n",
    "---\n",
    "\n",
    "### Let see it once again\n",
    "\n",
    "![](https://i.ibb.co/ZSs4NGz/Screenshot-2022-02-19-7-38-54-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### And now onwards to first building the index for the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "annotations = ['This protein phosporylates a gene','This protein catalyzes a reaction','This gene controls RNA expression', 'This gene controls a gene']\n",
    "\n",
    "annotwords = [[word for word in annotation.split()] for annotation in annotations]\n",
    "annotdict = corpora.Dictionary(annotwords)\n",
    "\n",
    "print(annotdict.token2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The index is created by genesim.corpora geting the \"bag of words\" (tokens)\n",
    "* From the bag of words (if there are replicated words) a unique set is created \n",
    "* This dictionary is used as positional reference for the 1's and 0's in the tensor\n",
    "\n",
    "---\n",
    "### Now onto building the tensor from the annotations and the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "annotensor = np.zeros(shape = (len(annotations), len(annotations[0].split(' ')), max(annotdict.token2id.values()) + 1))  \n",
    "\n",
    "for i, annotation in enumerate(annotations): \n",
    "  for j, word in list(enumerate(annotation.split())):\n",
    "    \n",
    "    index = annotdict.token2id.get(word)\n",
    "    annotensor[i, j, index] = 1.   \n",
    "\n",
    "print(annotations,\"\\n\")\n",
    "print(annotdict.token2id,\"\\n\")\n",
    "print(annotensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The \"enumerate\" allows us to have the counter (0,1,2..) and the element we iterate over\n",
    "* The \"i\" is for each matrix in the tensor storing one-hot encoding for a single annotation\n",
    "* The \"j\" are the rows of the matrix (words in annotation) and \"index\" the columns position on dictionary\n",
    "\n",
    "---\n",
    "\n",
    "### What has been achieved and some notes\n",
    "\n",
    "* We have encoding the annotations from human language to a standardized machine representation\n",
    "* The code we seen can be scaled to any size annotations, and thousands of them \n",
    "* Notice that annotations were of equal length, not the case with data we seen before\n",
    "* We either need to fix the data, find different data, or work with \"ragged\" tensors (possibly difficult)\n",
    "* We will return to the issue of getting the right data in subsequent sessions\n",
    "\n",
    "---\n",
    "\n",
    "### Next: the tip of the tip of the iceberg, for Graph Convolutional Networks\n",
    "\n",
    "* Graph Convolutional Networks - GCNs, a type of Artificial Neural Networks \n",
    "* We will see more Artificial Intelligence and Machine Learning (AI/ML) subsequently \n",
    "* Our focus is now to utilize vectorized, one-hot encoded annotation data on each node on our graphs\n",
    "* We will run a \"toy\" machine learning algorithm on the graph (prepare from matrix algebra !)\n",
    "\n",
    "---\n",
    "\n",
    "### Matrix Algebra refresher\n",
    "\n",
    "* We discussed [previously how adjacency matrices](https://www.ebi.ac.uk/training/online/courses/network-analysis-of-protein-interaction-data-an-introduction/introduction-to-graph-theory/graph-theory-adjacency-matrices/) represent the graph structure\n",
    "* Matrix multiplication is row to column, (k,l) x (l,m) = (k,m) inner dimensions must match\n",
    "* An alternative multiplication representation can be seen [here](https://www.geeksforgeeks.org/wp-content/uploads/strassen_new.png), multiple tutorials on the web \n",
    "\n",
    "![width:600](https://miro.medium.com/max/1400/1*YGcMQSr0ge_DGn96WnEkZw.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Tensor operations with numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(annotensor),\"\\n\")\n",
    "print(annotensor.shape,\"\\n\")\n",
    "\n",
    "sliceanot = np.sum(annotensor,axis=1)\n",
    "print(sliceanot,\"\\n\")\n",
    "\n",
    "print(sliceanot[0:4, 0:5],\"\\n\")\n",
    "\n",
    "anot = np.sum(sliceanot[0:4, 0:5],axis=1)\n",
    "anotvec = np.reshape(anot,[4,1])\n",
    "\n",
    "print(anot,\"\\n\")\n",
    "print(anotvec,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our annotensor variable is a [numpy array](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)\n",
    "* Crash course in numpy array slicing [here](https://www.earthdatascience.org/courses/intro-to-earth-data-science/scientific-data-structures-python/numpy-arrays/indexing-slicing-numpy-arrays/) and [here](https://www.pythoninformer.com/python-libraries/numpy/index-and-slice/)\n",
    "\n",
    "---\n",
    "\n",
    "* We created a compressed representation of example annotations and small tensors \n",
    "* Full scale annotations and tensors (with 1000's of entries) in real application \n",
    "* We naively represent each node feature (the 1's and 0's for the annotation encoding) as single number\n",
    "* We will demonstrate matrix operations for Graph Convolutions, following this [example tutorial](https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780) \n",
    "* Eventually we will use [Spectal](https://graphneural.network/) with large scale graphs and data\n",
    "\n",
    "![height:300](https://i.ibb.co/H2pq4sC/imgonline-com-ua-twotoone-KPNPbw-UW6-MEi-T.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A graph with the compressed annotations as node attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "A = np.matrix([\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1], \n",
    "    [0, 1, 0, 0],\n",
    "    [1, 0, 1, 0]],\n",
    "    dtype=float)\n",
    "\n",
    "G = nx.convert_matrix.from_numpy_matrix(A,create_using=nx.DiGraph)\n",
    "\n",
    "for i in G.nodes:\n",
    "\tG.nodes[i][\"annotval\"] = (int(anotvec[i]),'ID=' + str(i))\n",
    "\n",
    "labels = nx.get_node_attributes(G, 'annotval') \n",
    "nx.draw_shell(G, arrows=True, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Again, please refer to [NetworkX documentation](https://networkx.org/documentation/stable/index.html) for the functions we use here\n",
    "\n",
    "---\n",
    "\n",
    "### Update node attributes based on neighboors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = anotvec\n",
    "print(A,\"\\n\")\n",
    "print(X,\"\\n\")\n",
    "\n",
    "anotprop = A * X\n",
    "print(anotprop,\"\\n\")\n",
    "\n",
    "for i in G.nodes:\n",
    "\tG.nodes[i][\"annotval\"] = (int(anotprop[i]),'ID=' + str(i))\n",
    "\n",
    "labels = nx.get_node_attributes(G, 'annotval') \n",
    "nx.draw_shell(G, arrows=True, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is the basis for the message passing algorithm in Graph Convolutional neural Networks (GCNs)\n",
    "* Information propagation based on the structure of the graph, notice that after update we have sum of neighboors \n",
    "\n",
    "---\n",
    "\n",
    "### Adding self-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.matrix(np.eye(A.shape[0]))\n",
    "print(I,\"\\n\")\n",
    "\n",
    "A1 = A + I\n",
    "print(A1,\"\\n\")\n",
    "\n",
    "anotvecself = A1 * X\n",
    "\n",
    "for i in G.nodes:\n",
    "\tG.nodes[i][\"annotval\"] = (int(anotvecself[i]),'ID=' + str(i))\n",
    "\n",
    "labels = nx.get_node_attributes(G, 'annotval') \n",
    "nx.draw_shell(G, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With addition of the identity matrix we add self-loops in the graph\n",
    "* This allow us to sum up also the self-value of each node\n",
    "\n",
    "---\n",
    "### A summary and where we are going next\n",
    "\n",
    "* We have seen how to one-hot encode natural language annotations\n",
    "* Using over-simplified annotation vectors and added them as \"features\" on nodes of the graph\n",
    "* Then with a simple matrix multiplication how to propagate these features along the graph structure\n",
    "* A couple more matrix / linear algebra tricks will give us the formula / algorithm below\n",
    "* And next will be this algorithm in [Spektral](https://graphneural.network/layers/convolution/#gcnconv) where we use the full one-hot vectors and large-scale graphs\n",
    "\n",
    "![](https://i.ibb.co/Qf7DtC2/Screenshot-2022-02-22-10-42-34-PM.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
